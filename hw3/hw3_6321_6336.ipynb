{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide by\n",
    "\n",
    "นายธนชาติ เสถียรจารุการ 63340500021 <br>\n",
    "\n",
    "นายพชพล เพชรรัตน์ 63340500036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import math\n",
    "\n",
    "RandomState = 1\n",
    "df = pd.read_csv('credit_card_churn.csv')\n",
    "df = df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 'CLIENTNUM'], axis=1)\n",
    "df = df.rename(columns={'Attrition_Flag': 'y',\n",
    "                         'Customer_Age': 'age',\n",
    "                         'Gender': 'gender',\n",
    "                         'Dependent_count': 'dependency',\n",
    "                         'Education_Level': 'education',\n",
    "                         'Marital_Status': 'marital',\n",
    "                         'Income_Category': 'income',\n",
    "                         'Card_Category': 'card',\n",
    "                         'Months_on_book': 'book_period',\n",
    "                         'Total_Relationship_Count': 'total_product',\n",
    "                         'Months_Inactive_12_mon': 'month_inactive',\n",
    "                         'Contacts_Count_12_mon': 'contact_num',\n",
    "                         'Credit_Limit': 'credit_limit',\n",
    "                         'Total_Revolving_Bal': 'revolving_balance',\n",
    "                         'Avg_Open_To_Buy': 'open2buy',\n",
    "                         'Total_Amt_Chng_Q4_Q1': 'transaction_change',\n",
    "                         'Total_Trans_Amt': 'transaction_amount',\n",
    "                         'Total_Trans_Ct': 'transaction_count',\n",
    "                         'Total_Ct_Chng_Q4_Q1': 'transaction_count_change',\n",
    "                         'Avg_Utilization_Ratio': 'utilization_ratio',\n",
    "                         }\n",
    ")\n",
    "df['y'] = df['y'].replace(['Attrited Customer', 'Existing Customer'], [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target\n",
    "The total number of data is 10,127 samples devided into\n",
    "- Attrited: 8,500 samples\n",
    "<br>\n",
    "\n",
    "- Existing: 1,627 samples\n",
    "\n",
    "**the dataset is imbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['y'].value_counts(); print(count)\n",
    "plt.figure(figsize = (4, 3))\n",
    "sns.histplot(data = df, x = 'y')\n",
    "plt.show()\n",
    "print(f\"major target: {(count[1]/(count[0] + count[1])) * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### resample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_target = df.loc[df['y'] == 1]\n",
    "minor_target = df.loc[df['y'] == 0]\n",
    "upsampling_df = resample(minor_target, n_samples=major_target.shape[0], replace=True, random_state=RandomState)\n",
    "df = pd.concat([major_target, upsampling_df], ignore_index=True)\n",
    "df = df.sample(frac = 1, ignore_index=True)\n",
    "\n",
    "count = df['y'].value_counts(); print(count)\n",
    "plt.figure(figsize = (4, 3))\n",
    "sns.histplot(data = df, x = 'y')\n",
    "plt.show()\n",
    "\n",
    "print(f\"major target: {(count[1]/(count[0] + count[1])) * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore numerical data\n",
    "\n",
    "the data that seem to have an outlier is listed below\n",
    "- age\n",
    "- transaction_change\n",
    "- transaction_count_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperateDataType(df): ## return list of numerical and categorical data\n",
    "    cols = df.columns\n",
    "    num_data = [i for i in cols if (len(df[i].unique()) > 7 and df[i].dtype != 'object')]\n",
    "    cat_data = list(set(cols) - set(num_data))\n",
    "    return num_data, cat_data\n",
    "\n",
    "num_data, cat_data = seperateDataType(df)\n",
    "plot_num = 1\n",
    "plt.figure(figsize = (15,27))\n",
    "for i in num_data:\n",
    "    ax = plt.subplot(7, 3, plot_num)\n",
    "    sns.histplot(data=df, x=i, hue='y', kde=True)\n",
    "    plot_num += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use IQR to remove outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNumericalOutlier(df:pd.DataFrame, feature_list:list):\n",
    "    new_df = df.copy()\n",
    "    for feature in feature_list:       \n",
    "        q1 = df[feature].quantile(0.25)\n",
    "        q3 = df[feature].quantile(0.75)\n",
    "        IQR = q3 - q1\n",
    "        lower_bound = q1 - 1.5*IQR\n",
    "        upper_bound = q3 + 1.5*IQR\n",
    "        new_df = new_df[(new_df[feature]>lower_bound)&(new_df[feature]<upper_bound)]\n",
    "    return new_df\n",
    "\n",
    "outlier_list = ['age', 'transaction_change', 'transaction_count_change']\n",
    "df = removeNumericalOutlier(df, outlier_list)\n",
    "\n",
    "plot_num = 1\n",
    "plt.figure(figsize = (9, 4))\n",
    "for i in outlier_list:\n",
    "    ax = plt.subplot(1, 3, plot_num)\n",
    "    sns.histplot(data=df, x=i, hue='y')\n",
    "    plot_num += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use quantile transformer to correct skewed data and standardlize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctSkewed(df:pd.DataFrame, skewed_data_list:list):\n",
    "    ##### correct skewed data\n",
    "    quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=1)\n",
    "    x_skew = df[skewed_data_list].values\n",
    "    X_trans = quantile_transformer.fit_transform(x_skew)\n",
    "    df[skewed_data_list] = X_trans\n",
    "    return df\n",
    "\n",
    "df = correctSkewed(df, num_data)\n",
    "plot_num = 1\n",
    "plt.figure(figsize = (15,27))\n",
    "for i in num_data:\n",
    "    ax = plt.subplot(7, 3, plot_num)\n",
    "    sns.histplot(data=df, x=i, hue='y', kde=True)\n",
    "    plot_num += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot heatmap of numerical data correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df[num_data]\n",
    "num_df['y'] = df['y']\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(round(num_df.corr(), 2), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- visualize high correlation to target\n",
    "<br>\n",
    "\n",
    "*the distribution of some data that have a high correlation to the target seems to be the same value as listed below but the pair between Transaction_amount and transaction_count we not sure how this relation effect to model and we consider testing in the training model session*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_data_list = ['transaction_count', 'transaction_count_change', 'utilization_ratio', 'transaction_amount', 'revolving_balance']\n",
    "plot_num = 1\n",
    "plt.figure(figsize = (15,27))\n",
    "for i in high_corr_data_list:\n",
    "    ax = plt.subplot(7, 3, plot_num)\n",
    "    sns.boxplot(data=num_df, x='y', y=i)\n",
    "    plot_num += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- visualize relation between numerical data\n",
    "\n",
    "*the result is related to the correlation heatmap above, the pair with high correlation for each other show some mathematical function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=num_df, hue='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 1\n",
    "plt.figure(figsize = (15,27))\n",
    "for i in cat_data:\n",
    "    ax = plt.subplot(7, 3, plot_num)\n",
    "    sns.histplot(data=df, x=i, hue='y')\n",
    "    plot_num += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- group card category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['card'] = df['card'].replace(['Silver', 'Gold', 'Platinum'], ['not_blue']*3)\n",
    "plt.figure(figsize = (3,4))\n",
    "sns.histplot(data=df, x='card', hue='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- plot heatmap of categorial data correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = df[cat_data]\n",
    "cat_df = cat_df.drop(['y'],axis=1)\n",
    "cat_df = pd.get_dummies(cat_df)\n",
    "cat_df['y'] = df['y']\n",
    "plt.figure(figsize = (32,18))\n",
    "sns.heatmap(round(cat_df.corr(), 2), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- visualize high correlation to target\n",
    "<br>\n",
    "\n",
    "*the feature 'contact_num' and 'month_inactive' some labels have a high ratio of the target but the feature 'total_product' is not obviously show*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_data_list = ['contact_num', 'month_inactive', 'total_product']\n",
    "plot_num = 1\n",
    "plt.figure(figsize = (15,27))\n",
    "for i in high_corr_data_list:\n",
    "    ax = plt.subplot(7, 3, plot_num)\n",
    "    sns.countplot(data=cat_df, x=i, hue='y')\n",
    "    plot_num += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- visualize relation between categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 1\n",
    "plt.figure(figsize = (60,60))\n",
    "for i in cat_data:\n",
    "    for j in cat_data:\n",
    "        ax = plt.subplot(10, 10, plot_num)\n",
    "        sns.countplot(data=df, x=i, hue=j)\n",
    "        plot_num += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### visualize relation between categoriacal and numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 1\n",
    "plt.figure(figsize = (60,60))\n",
    "for i in cat_data:\n",
    "    for j in num_data:\n",
    "        ax = plt.subplot(10, 10, plot_num)\n",
    "        sns.boxplot(data=df, x=i, y=j ,hue='y')\n",
    "        plot_num += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data exploration summary\n",
    "\n",
    "1. upsampling data\n",
    "\n",
    "2. use IQR to remove outlier\n",
    "\n",
    "3. use quantile transformer to correct skewed data and standardlize\n",
    "\n",
    "4. list of numerical features below have a high correlation to the target but when we visualize one thing we realized is some features have close value for each other\n",
    "    * ['transaction_count', 'transaction_count_change', 'utilization_ratio', 'transaction_amount', 'revolving_balance']\n",
    "\n",
    "<br>\n",
    "<br\\>\n",
    "\n",
    "5. list of pair that have the relation between numerical data\n",
    "    - Utilization_ratio and revolving_balance\n",
    "    - Transaction_count and transaction_amount\n",
    "    - Book_period and age\n",
    "    - Open2buy and credit_limit\n",
    "\n",
    "<br>\n",
    "<br\\>\n",
    "\n",
    "6. list of categorical features below have high correlation to the target\n",
    "    - ['contact_num', 'month_inactive', 'total_product']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import classification_report, silhouette_score\n",
    "\n",
    "RandomState = 1\n",
    "\n",
    "def importDataset():\n",
    "    df = pd.read_csv('credit_card_churn.csv')\n",
    "    df = df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 'CLIENTNUM'], axis=1)\n",
    "    df = df.rename(columns={'Attrition_Flag': 'y',\n",
    "                         'Customer_Age': 'age',\n",
    "                         'Gender': 'gender',\n",
    "                         'Dependent_count': 'dependency',\n",
    "                         'Education_Level': 'education',\n",
    "                         'Marital_Status': 'marital',\n",
    "                         'Income_Category': 'income',\n",
    "                         'Card_Category': 'card',\n",
    "                         'Months_on_book': 'book_period',\n",
    "                         'Total_Relationship_Count': 'total_product',\n",
    "                         'Months_Inactive_12_mon': 'month_inactive',\n",
    "                         'Contacts_Count_12_mon': 'contact_num',\n",
    "                         'Credit_Limit': 'credit_limit',\n",
    "                         'Total_Revolving_Bal': 'revolving_balance',\n",
    "                         'Avg_Open_To_Buy': 'open2buy',\n",
    "                         'Total_Amt_Chng_Q4_Q1': 'transaction_change',\n",
    "                         'Total_Trans_Amt': 'transaction_amount',\n",
    "                         'Total_Trans_Ct': 'transaction_count',\n",
    "                         'Total_Ct_Chng_Q4_Q1': 'transaction_count_change',\n",
    "                         'Avg_Utilization_Ratio': 'utilization_ratio',\n",
    "                         }\n",
    "    )\n",
    "    df['y'] = df['y'].replace(['Attrited Customer', 'Existing Customer'], [0, 1])\n",
    "    return df\n",
    "\n",
    "def seperateDataType(df): ## return list of numerical and categorical data\n",
    "    cols = df.columns\n",
    "    num_data = [i for i in cols if (len(df[i].unique()) > 7 and df[i].dtype != 'object')]\n",
    "    cat_data = list(set(cols) - set(num_data))\n",
    "    return num_data, cat_data\n",
    "\n",
    "def reSampling(df:pd.DataFrame, up_flag:bool, down_flag:bool):\n",
    "    major_target = df.loc[df['y'] == 1]\n",
    "    minor_target = df.loc[df['y'] == 0]\n",
    "    new_df = df.copy()\n",
    "    if up_flag:\n",
    "        upsampling_df = resample(minor_target, n_samples=major_target.shape[0], replace=True, random_state=RandomState)\n",
    "        new_df = pd.concat([major_target, upsampling_df], ignore_index=True)\n",
    "        new_df = new_df.sample(frac = 1, ignore_index=True)\n",
    "    elif down_flag:\n",
    "        downsampling_df = resample(major_target, n_samples=minor_target.shape[0], replace=True, random_state=RandomState)\n",
    "        new_df = pd.concat([minor_target, downsampling_df], ignore_index=True)\n",
    "        new_df = new_df.sample(frac = 1, ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def removeOutlier(df:pd.DataFrame, feature_list:list, flag:bool):\n",
    "    new_df = df.copy()\n",
    "    if flag:\n",
    "        for feature in feature_list:\n",
    "            q1 = df[feature].quantile(0.25)\n",
    "            q3 = df[feature].quantile(0.75)\n",
    "            IQR = q3 - q1\n",
    "            lower_bound = q1 - 1.5*IQR\n",
    "            upper_bound = q3 + 1.5*IQR\n",
    "            new_df = new_df[(new_df[feature]>lower_bound)&(new_df[feature]<upper_bound)]\n",
    "    return new_df\n",
    "\n",
    "def correctSkewed(df:pd.DataFrame, skewed_data_list:list, flag:bool):\n",
    "    new_df = df.copy()\n",
    "    if flag:\n",
    "        quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=1)\n",
    "        x_skew = new_df[skewed_data_list].values\n",
    "        X_trans = quantile_transformer.fit_transform(x_skew)\n",
    "        new_df[skewed_data_list] = X_trans\n",
    "    return new_df\n",
    "\n",
    "def preprocessData(drop_list:list, \n",
    "                    select_list:list, \n",
    "                    upsampling_flag:bool = True, \n",
    "                    downsampling_flag:bool = True, \n",
    "                    remove_outlier_flag:bool = True, \n",
    "                    correct_skewed_flag:bool = True):\n",
    "    df = importDataset()\n",
    "    df = reSampling(df, upsampling_flag, downsampling_flag)\n",
    "    outlier_list = ['age', 'transaction_change', 'transaction_count_change']\n",
    "    df = removeOutlier(df, outlier_list, remove_outlier_flag)\n",
    "    num_data, cat_data = seperateDataType(df)\n",
    "    df = correctSkewed(df, num_data, correct_skewed_flag)\n",
    "    if drop_list is not None:\n",
    "        df = df.drop(drop_list, axis=1)\n",
    "    elif select_list is not None:\n",
    "        df = df[select_list]\n",
    "    df_y = df['y']\n",
    "    df_x = df.drop(['y'], axis=1)\n",
    "    df_x = pd.get_dummies(df_x)\n",
    "    x = df_x.values\n",
    "    y = df_y.values\n",
    "    scaler = StandardScaler()\n",
    "    x_scale = scaler.fit_transform(x)\n",
    "    return x_scale, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainK_mean(drop_list:list, \n",
    "                    select_list:list, \n",
    "                    upsampling_flag:bool = True, \n",
    "                    downsampling_flag:bool = True, \n",
    "                    remove_outlier_flag:bool = True, \n",
    "                    correct_skewed_flag:bool = True):\n",
    "    ##### get x and y data from preprocessing\n",
    "    x, y = preprocessData(drop_list, select_list, upsampling_flag, downsampling_flag, remove_outlier_flag, correct_skewed_flag)\n",
    "    ##### Split train and test data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)\n",
    "    ##### Declare model and model parameters\n",
    "    model = KMeans(n_clusters=2, random_state=1, max_iter=1000)\n",
    "    parameters = {'init':('k-means++', 'random'),\n",
    "                  'algorithm':(\"auto\", \"full\", \"elkan\")}\n",
    "    ##### Train model with gridsearchCV and split k-fold = 10\n",
    "    clf = GridSearchCV(model, parameters, scoring='f1', verbose=3, return_train_score=True, n_jobs=-1, cv = 10)\n",
    "    clf.fit(x_train, y_train)\n",
    "    ##### Test model, visualize classification report and confusion matrix\n",
    "    print(clf.best_params_)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"K means Classification report \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"K means Confusion matrix \\n=======================\")\n",
    "    return clf\n",
    "\n",
    "def trainAgglomerative(drop_list:list, \n",
    "                    select_list:list, \n",
    "                    upsampling_flag:bool = True, \n",
    "                    downsampling_flag:bool = True, \n",
    "                    remove_outlier_flag:bool = True, \n",
    "                    correct_skewed_flag:bool = True):\n",
    "    ##### get x and y data from preprocessing\n",
    "    x, y = preprocessData(drop_list, select_list, upsampling_flag, downsampling_flag, remove_outlier_flag, correct_skewed_flag)\n",
    "    ##### Declare model and model parameters\n",
    "    model = AgglomerativeClustering(n_clusters=2, compute_distances=True)\n",
    "    y_pred = model.fit_predict(x, y)\n",
    "    print(\"Agglomerative Clustering Classification report \\n=======================\")\n",
    "    print(classification_report(y_true=y, y_pred=y_pred))\n",
    "    print(\"Agglomerative Clustering Confusion matrix \\n=======================\")\n",
    "    return model\n",
    "\n",
    "def trainDBSCAN(drop_list:list, \n",
    "                    select_list:list, \n",
    "                    upsampling_flag:bool = True, \n",
    "                    downsampling_flag:bool = True, \n",
    "                    remove_outlier_flag:bool = True, \n",
    "                    correct_skewed_flag:bool = True):\n",
    "    ##### get x and y data from preprocessing\n",
    "    x, y = preprocessData(drop_list, select_list, upsampling_flag, downsampling_flag, remove_outlier_flag, correct_skewed_flag)\n",
    "    ##### Declare model and model parameters\n",
    "    for i in range(1,100,1):\n",
    "        model = DBSCAN(eps=i, n_jobs=-1)\n",
    "        y_pred = model.fit_predict(x, y)\n",
    "        if(max(y_pred) == 1):\n",
    "            print('eps= ' + str(i))\n",
    "            y_pred = model.fit_predict(x, y)\n",
    "            print(\"Agglomerative Clustering Classification report \\n=======================\")\n",
    "            print(classification_report(y_true=y, y_pred=y_pred))\n",
    "            print(\"Agglomerative Clustering Confusion matrix \\n=======================\")\n",
    "            break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXP 1 preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "{'algorithm': 'auto', 'init': 'k-means++'}\n",
      "K means Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.45      0.23       331\n",
      "           1       0.83      0.53      0.64      1695\n",
      "\n",
      "    accuracy                           0.51      2026\n",
      "   macro avg       0.49      0.49      0.44      2026\n",
      "weighted avg       0.72      0.51      0.58      2026\n",
      "\n",
      "K means Confusion matrix \n",
      "=======================\n",
      "Agglomerative Clustering Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.93      0.27      1627\n",
      "           1       0.84      0.07      0.13      8500\n",
      "\n",
      "    accuracy                           0.21     10127\n",
      "   macro avg       0.50      0.50      0.20     10127\n",
      "weighted avg       0.73      0.21      0.15     10127\n",
      "\n",
      "Agglomerative Clustering Confusion matrix \n",
      "=======================\n",
      "Agglomerative Clustering Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      1.00      0.28      1627\n",
      "           1       0.75      0.00      0.00      8500\n",
      "\n",
      "    accuracy                           0.16     10127\n",
      "   macro avg       0.46      0.50      0.14     10127\n",
      "weighted avg       0.66      0.16      0.05     10127\n",
      "\n",
      "Agglomerative Clustering Confusion matrix \n",
      "=======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DBSCAN(eps=11, n_jobs=-1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = None\n",
    "select_list = None\n",
    "trainK_mean(drop_list, select_list, upsampling_flag = False, downsampling_flag = False, remove_outlier_flag = False, correct_skewed_flag = False)\n",
    "trainAgglomerative(drop_list, select_list, upsampling_flag = False, downsampling_flag = False, remove_outlier_flag = False, correct_skewed_flag = False)\n",
    "trainDBSCAN(drop_list, select_list, upsampling_flag = False, downsampling_flag = False, remove_outlier_flag = False, correct_skewed_flag = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "{'algorithm': 'auto', 'init': 'random'}\n",
      "K means Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.57      0.55      1711\n",
      "           1       0.53      0.49      0.51      1689\n",
      "\n",
      "    accuracy                           0.53      3400\n",
      "   macro avg       0.53      0.53      0.53      3400\n",
      "weighted avg       0.53      0.53      0.53      3400\n",
      "\n",
      "K means Confusion matrix \n",
      "=======================\n",
      "Agglomerative Clustering Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.93      0.65      8500\n",
      "           1       0.50      0.07      0.12      8500\n",
      "\n",
      "    accuracy                           0.50     17000\n",
      "   macro avg       0.50      0.50      0.39     17000\n",
      "weighted avg       0.50      0.50      0.39     17000\n",
      "\n",
      "Agglomerative Clustering Confusion matrix \n",
      "=======================\n",
      "Agglomerative Clustering Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67      8500\n",
      "           1       0.38      0.00      0.00      8500\n",
      "\n",
      "    accuracy                           0.50     17000\n",
      "   macro avg       0.44      0.50      0.33     17000\n",
      "weighted avg       0.44      0.50      0.33     17000\n",
      "\n",
      "Agglomerative Clustering Confusion matrix \n",
      "=======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DBSCAN(eps=11, n_jobs=-1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = None\n",
    "select_list = None\n",
    "trainK_mean(drop_list, select_list, upsampling_flag = True, downsampling_flag = False, remove_outlier_flag = False, correct_skewed_flag = False)\n",
    "trainAgglomerative(drop_list, select_list, upsampling_flag = True, downsampling_flag = False, remove_outlier_flag = False, correct_skewed_flag = False)\n",
    "trainDBSCAN(drop_list, select_list, upsampling_flag = True, downsampling_flag = False, remove_outlier_flag = False, correct_skewed_flag = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "{'algorithm': 'auto', 'init': 'k-means++'}\n",
      "K means Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.41      0.45       340\n",
      "           1       0.45      0.53      0.49       311\n",
      "\n",
      "    accuracy                           0.47       651\n",
      "   macro avg       0.47      0.47      0.47       651\n",
      "weighted avg       0.47      0.47      0.47       651\n",
      "\n",
      "K means Confusion matrix \n",
      "=======================\n",
      "Agglomerative Clustering Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.93      0.65      1627\n",
      "           1       0.53      0.08      0.13      1627\n",
      "\n",
      "    accuracy                           0.50      3254\n",
      "   macro avg       0.52      0.50      0.39      3254\n",
      "weighted avg       0.52      0.50      0.39      3254\n",
      "\n",
      "Agglomerative Clustering Confusion matrix \n",
      "=======================\n",
      "Agglomerative Clustering Classification report \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67      1627\n",
      "           1       0.55      0.00      0.01      1627\n",
      "\n",
      "    accuracy                           0.50      3254\n",
      "   macro avg       0.52      0.50      0.34      3254\n",
      "weighted avg       0.52      0.50      0.34      3254\n",
      "\n",
      "Agglomerative Clustering Confusion matrix \n",
      "=======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DBSCAN(eps=11, n_jobs=-1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = None\n",
    "select_list = None\n",
    "trainK_mean(drop_list, select_list, upsampling_flag = False, downsampling_flag = True, remove_outlier_flag = False, correct_skewed_flag = False)\n",
    "trainAgglomerative(drop_list, select_list, upsampling_flag = False, downsampling_flag = True, remove_outlier_flag = False, correct_skewed_flag = False)\n",
    "trainDBSCAN(drop_list, select_list, upsampling_flag = False, downsampling_flag = True, remove_outlier_flag = False, correct_skewed_flag = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXP 2 Select good feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXP 3 Cut bad feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('FRA503': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64dadede5b611bea0003f916b3ffa8d6ffd0cb12e9e46b2f5e54ff5aa5c7df92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
