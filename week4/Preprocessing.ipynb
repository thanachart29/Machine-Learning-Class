{"cells":[{"cell_type":"markdown","metadata":{"id":"fS7T3M7ajKS5"},"source":["# Preprocessing Code Example\n","\n","### Part I: Set ups\n","This part includes the imported libraries and dataset. Feel free to change the path of the dataset accordingly."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1646099820779,"user":{"displayName":"KHANARSA PAISIT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMOtiPyB15RCrAFs4TOb-DYyAmLGaUyyTnV4xK=s64","userId":"00253122835342063556"},"user_tz":-420},"id":"E7btxShRYOxy"},"outputs":[{"name":"stdout","output_type":"stream","text":["c:\\Users\\thana\\Desktop\\TheShit\\FRA503\\class\\week4\n"]}],"source":["import pandas as pd\n","import os\n","import numpy as np\n","import sklearn.preprocessing\n","import seaborn as sns\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"1UfPWDzhYjMf"},"outputs":[],"source":["df = pd.read_csv('adult.data', header = None)\n","df.columns = ['age', 'workclass', 'fnlwgt', 'edu', 'edu-num', 'marital', 'occupation', 'relationship', 'race', 'sex', 'cap-gain', 'cap-loss','hpw','native country','income']\n","#a = df.loc[df['workclass'] == ' Private']\n","#c = a['income']\n","#df['workclass']\n","#df3 = df.drop(['workclass', 'edu', 'marital', 'occupation', 'race', 'relationship', 'sex', 'native country'], axis=1)\n","#sns.pairplot(df, hue = 'income')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Index([' <=50K', ' >50K'], dtype='object')\n"," <=50K\n"," Private             17733\n"," Self-emp-not-inc     1817\n"," ?                    1645\n"," Local-gov            1476\n"," State-gov             945\n"," Federal-gov           589\n"," Self-emp-inc          494\n"," Without-pay            14\n"," Never-worked            7\n","Name: workclass, dtype: int64\n"," >50K\n"," Private             4963\n"," Self-emp-not-inc     724\n"," Self-emp-inc         622\n"," Local-gov            617\n"," Federal-gov          371\n"," State-gov            353\n"," ?                    191\n","Name: workclass, dtype: int64\n"]}],"source":["index = df['income'].value_counts()\n","index_list = index.index\n","print(index_list)\n","\n","for i in range(len(index_list)):\n","    income = str(index_list[i])\n","    a = df.loc[df['income'] == income]\n","    c = a['workclass']\n","    print(income)\n","    print(c.value_counts())"]},{"cell_type":"markdown","metadata":{"id":"bd1kDVG5Uig0"},"source":["### Part II: Basic Data Understanding\n","\n","We can look at all values and their counts by using the following code. It is useful to understand all aspect of the dataset."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"WizUsTOUcyM9"},"outputs":[{"name":"stdout","output_type":"stream","text":["age\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","workclass\n","       age workclass  fnlwgt            edu  edu-num                 marital  \\\n","27      54         ?  180211   Some-college       10      Married-civ-spouse   \n","61      32         ?  293936        7th-8th        4   Married-spouse-absent   \n","69      25         ?  200681   Some-college       10           Never-married   \n","77      67         ?  212759           10th        6      Married-civ-spouse   \n","106     17         ?  304873           10th        6           Never-married   \n","...    ...       ...     ...            ...      ...                     ...   \n","32530   35         ?  320084      Bachelors       13      Married-civ-spouse   \n","32531   30         ?   33811      Bachelors       13           Never-married   \n","32539   71         ?  287372      Doctorate       16      Married-civ-spouse   \n","32541   41         ?  202822        HS-grad        9               Separated   \n","32542   72         ?  129912        HS-grad        9      Married-civ-spouse   \n","\n","      occupation    relationship                 race      sex  cap-gain  \\\n","27             ?         Husband   Asian-Pac-Islander     Male         0   \n","61             ?   Not-in-family                White     Male         0   \n","69             ?       Own-child                White     Male         0   \n","77             ?         Husband                White     Male         0   \n","106            ?       Own-child                White   Female     34095   \n","...          ...             ...                  ...      ...       ...   \n","32530          ?            Wife                White   Female         0   \n","32531          ?   Not-in-family   Asian-Pac-Islander   Female         0   \n","32539          ?         Husband                White     Male         0   \n","32541          ?   Not-in-family                Black   Female         0   \n","32542          ?         Husband                White     Male         0   \n","\n","       cap-loss  hpw  native country  income  \n","27            0   60           South    >50K  \n","61            0   40               ?   <=50K  \n","69            0   40   United-States   <=50K  \n","77            0    2   United-States   <=50K  \n","106           0   32   United-States   <=50K  \n","...         ...  ...             ...     ...  \n","32530         0   55   United-States    >50K  \n","32531         0   99   United-States   <=50K  \n","32539         0   10   United-States    >50K  \n","32541         0   32   United-States   <=50K  \n","32542         0   25   United-States   <=50K  \n","\n","[1836 rows x 15 columns]\n","fnlwgt\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","edu\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","edu-num\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","marital\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","occupation\n","       age workclass  fnlwgt            edu  edu-num                 marital  \\\n","27      54         ?  180211   Some-college       10      Married-civ-spouse   \n","61      32         ?  293936        7th-8th        4   Married-spouse-absent   \n","69      25         ?  200681   Some-college       10           Never-married   \n","77      67         ?  212759           10th        6      Married-civ-spouse   \n","106     17         ?  304873           10th        6           Never-married   \n","...    ...       ...     ...            ...      ...                     ...   \n","32530   35         ?  320084      Bachelors       13      Married-civ-spouse   \n","32531   30         ?   33811      Bachelors       13           Never-married   \n","32539   71         ?  287372      Doctorate       16      Married-civ-spouse   \n","32541   41         ?  202822        HS-grad        9               Separated   \n","32542   72         ?  129912        HS-grad        9      Married-civ-spouse   \n","\n","      occupation    relationship                 race      sex  cap-gain  \\\n","27             ?         Husband   Asian-Pac-Islander     Male         0   \n","61             ?   Not-in-family                White     Male         0   \n","69             ?       Own-child                White     Male         0   \n","77             ?         Husband                White     Male         0   \n","106            ?       Own-child                White   Female     34095   \n","...          ...             ...                  ...      ...       ...   \n","32530          ?            Wife                White   Female         0   \n","32531          ?   Not-in-family   Asian-Pac-Islander   Female         0   \n","32539          ?         Husband                White     Male         0   \n","32541          ?   Not-in-family                Black   Female         0   \n","32542          ?         Husband                White     Male         0   \n","\n","       cap-loss  hpw  native country  income  \n","27            0   60           South    >50K  \n","61            0   40               ?   <=50K  \n","69            0   40   United-States   <=50K  \n","77            0    2   United-States   <=50K  \n","106           0   32   United-States   <=50K  \n","...         ...  ...             ...     ...  \n","32530         0   55   United-States    >50K  \n","32531         0   99   United-States   <=50K  \n","32539         0   10   United-States    >50K  \n","32541         0   32   United-States   <=50K  \n","32542         0   25   United-States   <=50K  \n","\n","[1843 rows x 15 columns]\n","relationship\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","race\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","sex\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","cap-gain\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","cap-loss\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","hpw\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n","native country\n","       age          workclass  fnlwgt            edu  edu-num  \\\n","14      40            Private  121772      Assoc-voc       11   \n","38      31            Private   84154   Some-college       10   \n","51      18            Private  226956        HS-grad        9   \n","61      32                  ?  293936        7th-8th        4   \n","93      30            Private  117747        HS-grad        9   \n","...    ...                ...     ...            ...      ...   \n","32449   44       Self-emp-inc   71556        Masters       14   \n","32469   58       Self-emp-inc  181974      Doctorate       16   \n","32492   42   Self-emp-not-inc  217597        HS-grad        9   \n","32510   39            Private  107302        HS-grad        9   \n","32525   81                  ?  120478      Assoc-voc       11   \n","\n","                      marital       occupation    relationship  \\\n","14         Married-civ-spouse     Craft-repair         Husband   \n","38         Married-civ-spouse            Sales         Husband   \n","51              Never-married    Other-service       Own-child   \n","61      Married-spouse-absent                ?   Not-in-family   \n","93         Married-civ-spouse            Sales            Wife   \n","...                       ...              ...             ...   \n","32449      Married-civ-spouse            Sales         Husband   \n","32469           Never-married   Prof-specialty   Not-in-family   \n","32492                Divorced            Sales       Own-child   \n","32510      Married-civ-spouse   Prof-specialty         Husband   \n","32525                Divorced                ?       Unmarried   \n","\n","                      race      sex  cap-gain  cap-loss  hpw native country  \\\n","14      Asian-Pac-Islander     Male         0         0   40              ?   \n","38                   White     Male         0         0   38              ?   \n","51                   White   Female         0         0   30              ?   \n","61                   White     Male         0         0   40              ?   \n","93      Asian-Pac-Islander   Female         0      1573   35              ?   \n","...                    ...      ...       ...       ...  ...            ...   \n","32449                White     Male         0         0   50              ?   \n","32469                White   Female         0         0   99              ?   \n","32492                White     Male         0         0   50              ?   \n","32510                White     Male         0         0   45              ?   \n","32525                White   Female         0         0    1              ?   \n","\n","       income  \n","14       >50K  \n","38       >50K  \n","51      <=50K  \n","61      <=50K  \n","93      <=50K  \n","...       ...  \n","32449    >50K  \n","32469   <=50K  \n","32492   <=50K  \n","32510    >50K  \n","32525   <=50K  \n","\n","[583 rows x 15 columns]\n","income\n","Empty DataFrame\n","Columns: [age, workclass, fnlwgt, edu, edu-num, marital, occupation, relationship, race, sex, cap-gain, cap-loss, hpw, native country, income]\n","Index: []\n"]}],"source":["for i in df.columns:\n","    print(i)\n","    #print(df[str(i)].value_counts().index)\n","    print(df.loc[df[str(i)] == ' ?'])"]},{"cell_type":"markdown","metadata":{"id":"DdjUjjQbU7MM"},"source":["### Part III: Preprocessing\n","\n","The following lines of code show how you can normalize the data. Noted that if you want to normalize only one feature, you need to change the datatype accordingly. \n","\n","You can create a new dataframe with one feature by using old_df[['feature_name']]\n","\n","More info at https://scikit-learn.org/stable/modules/preprocessing.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Icmq9A7idxY2","outputId":"c2d36873-5ae3-45d1-ea99-2210b911f616"},"outputs":[],"source":["df2 = df[['age']]\n","\n","min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n","min_max_scaler.fit_transform(df2)"]},{"cell_type":"markdown","metadata":{"id":"CxiR1WUCVk5P"},"source":["Or you can change from array (df['feature_name']) to numpy array by using np.array()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cv6MFvT2fAw9"},"outputs":[],"source":["x_scaled2=min_max_scaler.fit_transform(np.array(df['age']).reshape(1,-1))\n","print(x_scaled2.shape)"]},{"cell_type":"markdown","metadata":{"id":"PGG1GaOrV_Jm"},"source":["To preform one-hot encoding for categorical feature, you can use the following lines of code.\n","\n","More info at https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npbP22FOgvoG"},"outputs":[],"source":["df3 = df[['edu']]\n","df4 = pd.get_dummies(df3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#df4['income'] = df['income'].values\n","#df4.head()\n","#sns.pairplot(df4, hue= 'income')"]},{"cell_type":"markdown","metadata":{"id":"CJih5j1sW93J"},"source":["You can also perform feature selection on the whole dataframe.\n","\n","More info at https://scikit-learn.org/stable/modules/feature_selection.html"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ThJPMM5rhmjG"},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","\n","x = df.iloc[:,:-1]    #Split only data\n","y = df.iloc[:,-1]     #Split the target out"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["0         <=50K\n","1         <=50K\n","2         <=50K\n","3         <=50K\n","4         <=50K\n","          ...  \n","32556     <=50K\n","32557      >50K\n","32558     <=50K\n","32559     <=50K\n","32560      >50K\n","Name: income, Length: 32561, dtype: object"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBlW-pZyXosf"},"outputs":[],"source":["# We can one-hot encode the dataframe. This line of code will encode only categorical features automatically.\n","x = pd.get_dummies(x)\n","\n","# We then create the feature selector. In this case, we use chi-2 algorithm and we want to choose 4 features (k=4).\n","selector = SelectKBest(chi2, k=4)     #This line creates the selector\n","x_new = selector.fit(x,y)             #This line fits the selector to the dataset, and select the features.\n","\n","# Once we fit the selector, all features are selected and its indices are saved. We can create a new dataframe with those indices.\n","col = selector.get_support(indices=True)   #all indices are saved in col.\n","x_new = x.iloc[:,col]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"CuLLri6pXvUq","outputId":"80030305-92e3-4bc4-cae0-8261b94a8a1d"},"outputs":[],"source":["x_new.head()"]},{"cell_type":"markdown","metadata":{"id":"6gWoQQ1uaS6A"},"source":["You can also perform feature extraction. \n","\n","More info at https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSfR9YFhX_Zi","outputId":"b0def55c-15d0-4c35-8cdb-33d2be1a4720"},"outputs":[{"ename":"ValueError","evalue":"could not convert string to float: ' State-gov'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15284\\4142350758.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# Create PCA transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx_pca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# Fit and transform PCA transformer to the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# This show the variance of each component.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\thana\\Desktop\\TheShit\\FRA503\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \"\"\"\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\thana\\Desktop\\TheShit\\FRA503\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         X = self._validate_data(\n\u001b[1;32m--> 431\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         )\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\thana\\Desktop\\TheShit\\FRA503\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\thana\\Desktop\\TheShit\\FRA503\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n","\u001b[1;32mc:\\Users\\thana\\Desktop\\TheShit\\FRA503\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m     def __array_wrap__(\n","\u001b[1;31mValueError\u001b[0m: could not convert string to float: ' State-gov'"]}],"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components = 4)           # Create PCA transformer\n","x_pca = pca.fit_transform(x)          # Fit and transform PCA transformer to the dataset\n","\n","print(pca.explained_variance_ratio_)  # This show the variance of each component."]},{"cell_type":"markdown","metadata":{"id":"GFTpu3R1bl8D"},"source":["The array above shows the explained variance of each component. The first element is 0.99511. It means that the first component can explain 99.51% of the dataset already. Thus, it means that we can reduce the dimension of the original dataset to only 1 feature that basically covers 99.51% of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"8xKAyBjxa22Q","outputId":"71bc4c05-114d-41ba-dbe6-b0c72196fc05"},"outputs":[],"source":["pd.DataFrame(x_pca).head()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Preprocessing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.7.9 ('FRA503': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"64dadede5b611bea0003f916b3ffa8d6ffd0cb12e9e46b2f5e54ff5aa5c7df92"}}},"nbformat":4,"nbformat_minor":0}
